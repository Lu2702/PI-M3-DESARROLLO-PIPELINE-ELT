# ğŸ“¦ Proyecto: Pipeline ELT Airbnb NYC â€” Batch + DWH + Airflow

## ğŸ§­ Resumen general
Este proyecto implementa un **pipeline ELT** para **ingesta, transformaciÃ³n y almacenamiento** de datos en **procesamiento por lotes (batch)**. El flujo completo se ejecuta de forma **orquestada con Apache Airflow** y persiste los datos en un **Data Warehouse escalable y estructurado por capas** (staging â†’ silver â†’ gold).

### Objetivos principales
- **DiseÃ±ar un pipeline ELT** para la ingesta, transformaciÃ³n y almacenamiento de datos en **batch**.
- **Construir un Data Warehouse escalable** y **estructurado en capas** (medallÃ³n).
- **Orquestar** el pipeline end-to-end con **Apache Airflow**.
- **Implementar procesos de ingesta** mediante **scripts en Python** (extractores modulares, logging, control de calidad y verificaciÃ³n de archivos).
- **Aplicar transformaciones** sobre grandes volÃºmenes utilizando **SQL** (modelado dimensional con dbt).
- **Orquestar integralmente** la ingesta, extracciÃ³n, transformaciÃ³n y persistencia mediante **workflows en Apache Airflow**.

---

## ğŸ“‚ Sobre la estructura del repositorio (importante)
En la **raÃ­z de este repositorio** encontrarÃ¡s una carpeta llamada **[PRIMER AVANCE/](PRIMER%20AVANCE/)**.  
Esta carpeta contiene la **primera entrega** con el **diseÃ±o de la arquitectura general** para llevar el pipeline a la **nube (AWS)**. AhÃ­ se documenta:

- Las **herramientas** a utilizar (contenedores, orquestaciÃ³n, almacenamiento, modelado).
- La **arquitectura objetivo en AWS**.
- La **estructura del proyecto** desde la **E (Extract)** hasta la **T (Transform)**, incluyendo **L (Load)**, con diagramas y justificaciÃ³n tÃ©cnica.
- Consideraciones de **gobernanza, calidad de datos y escalabilidad**.

Puedes **visualizar esta primera parte** en el PDF dentro de esa carpeta:

- ğŸ“„ **PDF**: [`PRIMER AVANCE/DiseÃ±o de la arquitectura general del pipeline ELT AB_NYC.pdf`](PRIMER%20AVANCE/Dise%C3%B1o%20de%20la%20arquitectura%20general%20del%20pipeline%20ELT%20AB_NYC.pdf)

---

## ğŸ—ï¸ Componentes del pipeline (visiÃ³n rÃ¡pida)
- **Extract (E)**: Scripts en **Python** para:
  - AB_NYC (CSV)
  - Banxico SIE (API FX USD/MXN)
  - Wikipedia (scraping de boroughs)
- **Load (L)**: Carga cruda a esquema **staging** y almacenamiento de archivos **raw** por fecha.
- **Transform (T)**: Modelado **dbt** en capas **staging â†’ silver â†’ gold** (incluye SCDs donde aplica).
- **OrquestaciÃ³n**: **Apache Airflow** (DAGs, dependencias)
- **Calidad de datos**: Reglas en Python + reportes por corrida (duplicados, rangos, dominios, nulidad).

# ğŸš€ Capa RAW â€“ Pipeline de ExtracciÃ³n en Contenedor `extractor`

La **capa RAW** es la primera etapa del pipeline de datos.  
AquÃ­ almacenamos los datos **sin transformar**, organizados por **fuente/aÃ±o/mes/dÃ­a**, junto con **metadatos, reportes de calidad y logs**.  

ğŸ‘‰ Toda la extracciÃ³n corre dentro de un **contenedor ligero** (`extractor`) definido en [`extractor.Dockerfile`](extractor.Dockerfile).  
ğŸ‘‰ Los logs se registran tanto en **consola** como en el archivo [`logs`](logs\extractor.log).  

---

## ğŸ—ï¸ Estructura del proyecto

proyecto_integrador_ETL/  
â”œâ”€â”€ data/                # ğŸ“‚ Carpeta de almacenamiento RAW  
â”œâ”€â”€ logs/                # ğŸ“ Logs del extractor  
â”‚   â””â”€â”€ extractor.log  
â”œâ”€â”€ src/                 # ğŸ“¦ CÃ³digo fuente del pipeline  
â”‚   â”œâ”€â”€ extract/         # ğŸ“¥ Extractores de cada fuente  
â”‚   â”‚   â”œâ”€â”€ extract_csv.py  
â”‚   â”‚   â”œâ”€â”€ extract_banxico.py  
â”‚   â”‚   â””â”€â”€ web_scraping_nyc.py  
â”‚   â”œâ”€â”€ utils/           # âš™ï¸ Utilidades compartidas  
â”‚   â”‚   â”œâ”€â”€ config.py  
â”‚   â”‚   â”œâ”€â”€ logger.py  
â”‚   â”‚   â”œâ”€â”€ paths.py  
â”‚   â”‚   â”œâ”€â”€ quality.py  
â”‚   â”‚   â””â”€â”€ verify.py  
â”‚   â””â”€â”€ main.py          # ğŸ¯ Orquestador de la capa RAW  
â”œâ”€â”€ inputs/              # ğŸ“‚ Archivos CSV de entrada (ej: AB_NYC.csv)  
â”œâ”€â”€ .env                 # ğŸŒ ConfiguraciÃ³n de entorno  
â”œâ”€â”€ extractor.Dockerfile # ğŸ“¦ Imagen ligera del extractor  
â”œâ”€â”€ docker-compose.yml   # ğŸ³ OrquestaciÃ³n de servicios  
â””â”€â”€ requirements.txt     # ğŸ“‹ Dependencias mÃ­nimas  


---

## ğŸ“– ExplicaciÃ³n de los archivos principales

### ğŸ”¹ Carpeta `extract/`
- **[`extract_csv.py`](src\extract\extract_csv.py)** ğŸ“„  
  Copia el CSV `AB_NYC` desde `inputs/` a RAW.  
  - Calcula MD5 â†’ si el archivo no cambiÃ³, **no lo copia** y registra referencia diaria en el manifest.  
  - Garantiza trazabilidad con **[`manifest_raw.jsonl`](data\status\verify\ab_nyc\manifest_raw.jsonl).**

- **[`extract_banxico.py`](src\extract\extract_banxico.py)** ğŸ’±  
  Descarga el tipo de cambio USD/MXN desde la API **Banxico SIE**.  
  - Normaliza en columnas: `fecha`, `valor`.  
  - Guarda en RAW con convenciÃ³n:  
    `data/raw/files/banxico/YYYY/MM/DD/banxico_<serie>_<timestamp>.csv`.
  - Calcula MD5 â†’ si el archivo no cambiÃ³, **no lo copia** y registra referencia diaria en el manifest.  
  - Garantiza trazabilidad con **[`manifest_raw.jsonl`](data\status\verify\banxico\manifest_raw.jsonl).**    

- **`web_scraping_nyc.py`** ğŸŒ  
  Scraping de Wikipedia â†’ tabla de **boroughs de NYC**.  
  - Limpieza de columnas: `borough`, `population`, `land_area_km2`, `density_km2`.  
  - Valida que existan los 5 boroughs esperados.  
  - Guarda en RAW bajo convenciÃ³n:  
    `data/raw/files/nyc_boroughs/YYYY/MM/DD/nyc_boroughs_<timestamp>.csv`.
  - Calcula MD5 â†’ si el archivo no cambiÃ³, **no lo copia** y registra referencia diaria en el manifest.  
  - Garantiza trazabilidad con **[`manifest_raw.jsonl`](data\status\verify\nyc_boroughs\manifest_raw.jsonl).** 

---

### ğŸ”¹ Carpeta `utils/`
- **[`config.py`](src\utils\config.py)** âš™ï¸  
  Carga `.env` automÃ¡ticamente. Define variables como:  
  `RAW_DIR`, `BANXICO_SERIES_ID`, `RUN_SCRAPER_NYC`, `LOG_LEVEL`.  

- **[`logger.py`](src\utils\logger.py)** ğŸ“  
  Logger Ãºnico con formato:  
- Consola + archivo.  

- **[`paths.py`](src\utils\paths.py)** ğŸ“  
Genera rutas estÃ¡ndar para RAW:  
`data/raw/files/<source>/<YYYY>/<MM>/<DD>`.  

- **[`quality.py`](src\utils\quality.py)** âœ…  
Define **reglas de calidad de datos (DQ)** para cada fuente.  
- Ejemplo `banxico`: columna `valor` > 0 y fechas Ãºnicas.  
- Ejemplo `ab_nyc`: precios â‰¥ 0, `room_type` vÃ¡lido.  
- Genera reportes JSON en [`data/status/dq/<source>/...`](data\status\dq\banxico\2025\09\02\dq_banxico_20250902T194639Z.json).  

- **[`verify.py`](src\utils\verify.py)** ğŸ”’  
- Calcula **MD5** y evita duplicados.  
- Registra en manifest: [`data/status/verify/<source>/manifest_raw.jsonl`](data\status\verify\banxico\manifest_raw.jsonl).  
- Soporta referencias diarias (si el archivo no cambiÃ³).  

---

### ğŸ”¹ Orquestador
- **[`main.py`](src\main.py)** ğŸ¯  
Orquesta los tres extractores:  
1. **CSV â†’ RAW** (con referencia diaria si no cambiÃ³).  
2. **Banxico â†’ RAW** (siempre crea archivo y se valida).  
3. **Scraper NYC â†’ RAW** (solo si `RUN_SCRAPER_NYC=1`).  

DespuÃ©s de cada paso:  
- Se valida DQ (`quality.py`).  
- Se aplica `_post_write` â†’ existencia, tamaÃ±o, MD5, deduplicaciÃ³n.  
- Si falla un step en modo **soft** (`STRICT_MODE=0`), se genera un **artefacto de estado** en `data/status/extract/...`.

## ğŸ“‘ Ejemplos de artefactos en la capa RAW

Para que quede claro cÃ³mo luce la informaciÃ³n registrada en `status/`, aquÃ­ algunos ejemplos reales:

---

### ğŸ”’ Manifest â€“ `data/status/verify/banxico/manifest_raw.jsonl`

Este archivo almacena un registro por cada CSV guardado en RAW, incluyendo su **hash MD5** para evitar duplicados.

```json
{"ts_utc": "2025-09-04T12:00:00Z", "source": "banxico", "path": "data/raw/files/banxico/2025/09/04/banxico_SF43718_20250904T120000Z.csv", "md5": "5f3c8d9c7eae45f6d28c3f29a63b91de"}
{"ts_utc": "2025-09-04T12:05:00Z", "source": "banxico", "path": "data/raw/files/banxico/2025/09/04/banxico_SF43718_20250904T120500Z.csv", "md5": "5f3c8d9c7eae45f6d28c3f29a63b91de", "reference": true}
```

## âš™ï¸ Archivos de infraestructura
### ğŸŒ Ejemplo de `.env`

Este archivo configura rutas, tokens y flags de ejecuciÃ³n:

```env
# ============ Logging & rutas ============
LOG_LEVEL=INFO
RAW_DIR=./data/raw

# ============ CSV local de ejemplo ============
LOCAL_CSV_PATH=./inputs/AB_NYC.csv
LOCAL_CSV_SOURCE_NAME=ab_nyc

# ============ Banxico ============
BANXICO_SERIES_ID=SF43718
BANXICO_TOKEN=<tu_token_aqui>
BANXICO_SOURCE_NAME=banxico

# PolÃ­tica de fallo global: 0=soft-fail, 1=fail-fast
STRICT_MODE=0

# ============ Scraper Wikipedia (boroughs NYC) ============
RUN_SCRAPER_NYC=1
SCRAPER_NYC_SOURCE_NAME=nyc_boroughs
SCRAPER_NYC_URL=https://en.wikipedia.org/wiki/Boroughs_of_New_York_City

# ================ parÃ¡metros HTTP del scraper =================
HTTP_USER_AGENT=Integrador-ETL/1.0 (+educativo)
HTTP_TIMEOUT=30

# ============ Data Quality (DQ) ============
DQ_STRICT=0

# ================ Postgres (compose) ===================
POSTGRES_USER=nyc_user
POSTGRES_PASSWORD=nyc_pass
POSTGRES_DB=ab_nyc_dw
POSTGRES_HOST=postgres
POSTGRES_PORT=5433

# ==========airflow================
#AIRFLOW_DB=airflow
#TZ=America/Mexico_City

# ================== ConexiÃ³n local para notebooks ===================
PGHOST=localhost
PGPORT=5433
PGDATABASE=ab_nyc_dw
PGUSER=nyc_user
PGPASSWORD=nyc_pass
PGSCHEMA=public  
```
## ğŸ³ Contenedores: cÃ³mo funciona el `extractor` y cÃ³mo se ejecuta

La capa RAW corre dentro de un **contenedor ligero** llamado `extractor`.  
El contenedor se construye con [`extractor.Dockerfile`](extractor.Dockerfile) y se orquesta desde [`docker-compose.yml`](docker-compose.yml).

---

### ğŸ§± Â¿QuÃ© hace [`extractor.Dockerfile`](extractor.Dockerfile)? (idea general)

- **Base ligera de Python** (slim): reduce tamaÃ±o de la imagen y el tiempo de build.
- **Instala solo lo necesario** desde [`requirements.txt`](requirements.txt) (pandas, bs4, dotenv).
- Define el **directorio de trabajo** `/app`.
- Copia el **cÃ³digo fuente** `src/`, el archivo `.env` (si aplica en build) y la carpeta `inputs/` (o bien se montan en runtime via compose).

## ğŸš€ Ciclo de ejecuciÃ³n â€” Capa RAW (en contenedor `extractor`)

### âœ… Prerrequisitos (una sola vez)
- Tener **Docker** y **docker compose** instalados.
- Archivo **`.env`** en la raÃ­z con variables mÃ­nimas:
  - `RAW_DIR`, `LOG_LEVEL`
  - `LOCAL_CSV_PATH`, `LOCAL_CSV_SOURCE_NAME`
  - `BANXICO_SERIES_ID`, `BANXICO_TOKEN`
  - `RUN_SCRAPER_NYC` (0/1), `SCRAPER_NYC_URL`, `HTTP_USER_AGENT`, `HTTP_TIMEOUT`
- CSV de entrada disponible en `./inputs/` (ej. `AB_NYC.csv`).

---

### ğŸ§± 1) Construir la imagen ligera
```bash
docker compose build extractor
```
### â–¶ï¸ 2) Ejecutar el pipeline RAW (one-shot)
```bash
 docker compose run --rm extractor
# (equivalente) docker compose run --rm extractor python -m src.main
```
### ğŸ“‚ Verificar artefactos generados

```text
data/
â”œâ”€ raw/
â”‚  â””â”€ files/
â”‚     â””â”€ <source>/
â”‚        â””â”€ <YYYY>/<MM>/<DD>/
â”‚           â””â”€ <source>_<timestamp>.csv
â”œâ”€ status/
â”‚  â”œâ”€ verify/
â”‚  â”‚  â””â”€ <source>/manifest_raw.jsonl
â”‚  â”œâ”€ dq/
â”‚  â”‚  â””â”€ <source>/<YYYY>/<MM>/<DD>/
â”‚  â”‚     â””â”€ dq_<source>_<timestamp>.json
â”‚  â””â”€ extract/
â”‚     â””â”€ <stage>/<stage>_<timestamp>.json   # solo si hubo fallo soft
â””â”€ logs/
   â””â”€ extractor.log                          
```

# ğŸ§± Capa de TransformaciÃ³n (dbt) â€” **Load** & **Transform**

Esta secciÃ³n documenta **cÃ³mo pasamos de RAW â†’ DWH** usando **dbt** (modelo medallÃ³n: *staging â†’ silver â†’ gold*), partiendo de *snapshots* diarios y **foreign tables** que apuntan a los `latest.csv`.  
Antes de transformar, el diseÃ±o fue definido en el cuaderno **[`notebook/01_modelado_dimensional.ipynb`](notebook\01_modelado_dimensional.ipynb)**, donde se describe el **modelo dimensional** (hechos y dimensiones) y preguntas de negocio. 

---

## ğŸ§© VisiÃ³n general del flujo (con iconos)

```text
ğŸ“¥ RAW (archivos CSV por fuente, diarios)
   â””â”€â”€ data/raw/files/<source>/<YYYY>/<MM>/<DD>/<source>_<timestamp>.csv

ğŸ”— Symlinks "latest" (snapshot vigente del dÃ­a)
   â””â”€â”€ scripts/update_latest_symlinks.sh     # apunta <source>/latest.csv al CSV mÃ¡s reciente

ğŸ—„ï¸ Postgres (file_fdw â†’ FOREIGN TABLES)
   â””â”€â”€ raw_ext.ab_nyc_latest         â†’ data/raw/files/ab_nyc/latest.csv
   â””â”€â”€ raw_ext.banxico_latest        â†’ data/raw/files/banxico/latest.csv
   â””â”€â”€ raw_ext.nyc_boroughs_latest   â†’ data/raw/files/nyc_boroughs/latest.csv

ğŸ—ï¸ dbt SOURCES (leen raw_ext.*_latest)
   â””â”€â”€ sources.yml  # define "raw_ext" como fuente

ğŸ§± STAGING (tipificaciÃ³n, limpieza, snapshot keys)
   â””â”€â”€ models/staging/*.sql
       â€¢ Normaliza tipos/columnas
       â€¢ Agrega:
         - snapshot_date (DATE)
         - snapshot_date_key (INT: YYYYMMDD)

ğŸ“š SNAPSHOTS (SCD-2 cuando aplica)
   â””â”€â”€ snapshots/*.sql
       â€¢ Capturan cambios en el tiempo (por ejemplo listing, host)

ğŸ›ï¸ SILVER (dimensiones conformadas y hechos)
   â””â”€â”€ models/silver/*.sql
       â€¢ Dimensiones: dim_borough, dim_neighbourhood, dim_room_type, dim_host, dim_listing, dim_exchange_rate, â€¦
       â€¢ Hechos / auditorÃ­as: fct_listing_snapshot, fx_rate_audit, â€¦

ğŸ¥‡ GOLD (mÃ©tricas de negocio y agregados)
   â””â”€â”€ models/gold/gq*.sql
       â€¢ gq1_price_by_area, gq7_price_distribution_outliers, gq9_borough_supply_density_ranked, â€¦
```

---

## ğŸ“¦ **Load** (cargar â€œinstantÃ¡nea del dÃ­aâ€)

### 1) Mantener el **snapshot vigente**: `latest.csv`
El script [`scripts/update_latest_symlinks.sh`](scripts\update_latest_symlinks.sh) se ejecuta contra el contenedor de Postgres:
- Busca el **CSV mÃ¡s reciente** por fuente en `data/raw/files/<source>/YYYY/MM/DD/*.csv`.
- Actualiza el **symlink** `data/raw/files/<source>/latest.csv` para que siempre apunte al Ãºltimo archivo.

> Comando sugerido:  
> `bash scripts/update_latest_symlinks.sh`  
> *(si estÃ¡s en Windows con Docker Desktop, tambiÃ©n funciona al invocarlo desde Git Bash o WSL)*

---

### 2) Crear **bases de datos** y **foreign tables** (carpeta `sql/`)

- En [`sql/`](sql) se incluyen los scripts que **preparan Postgres**:
  - [`000_init.sh`](sql\000_init.sh): crea de forma idempotente las BDs del proyecto (ej. `ab_nyc_dw`, `airflow`). *(Ruta sugerida para el link: `sql/000_init.sh`)*  
  - [`010_raw_ext_foreign_tables.sql`](sql\010_raw_ext_foreign_tables.sql): crea el **schema** `raw_ext`, el **servidor** `csv_server` (`file_fdw`) y las **FOREIGN TABLES**:
    - `raw_ext.ab_nyc_latest` â†’ `/data/raw/files/ab_nyc/latest.csv`
    - `raw_ext.banxico_latest` â†’ `/data/raw/files/banxico/latest.csv`
    - `raw_ext.nyc_boroughs_latest` â†’ `/data/raw/files/nyc_boroughs/latest.csv` 

> **CÃ³mo se ejecutan:**  
> - Si `docker-compose.yml` monta `./sql:/docker-entrypoint-initdb.d:ro`, se aplican **automÃ¡ticamente** al levantar Postgres.  
> - Alternativa manual: `docker compose exec postgres psql -U <user> -d <db> -f /path/en/contenedor/010_raw_ext_foreign_tables.sql`

---

## ğŸ”§ **Transform** (dbt) â€” MedallÃ³n

### A) **Staging** (tipificaciÃ³n, normalizaciÃ³n y snapshot keys)
- Los modelos de *staging* toman `raw_ext.*_latest` como **source**.  
- Ejemplo: [`models/staging/stg_ab_nyc.sql`](ab_nyc_dw\models\staging\stg_ab_nyc.sql):
  - Tipifica y normaliza columnas (`id` â†’ `listing_id_nat`, `price` â†’ `price_usd`, `last_review` â†’ `last_review_date`, etc.).
  - **Enriquece con metadatos de snapshot**:
    - `snapshot_date` (date)
    - `snapshot_date_key` (int `YYYYMMDD`)
  - Estas claves se parametrizan vÃ­a `var('snapshot_date')` / `var('snapshot_date_key')` y por defecto toman la fecha de ejecuciÃ³n (`run_started_at`). 

> **EjecuciÃ³n recomendada (solo staging):**  
> `docker compose run --rm dbt dbt run --select staging`

> **Opcional (fijar snapshot del dÃ­a en una corrida histÃ³rica):**  
> `docker compose run --rm dbt dbt run --select staging --vars "snapshot_date: 2025-09-04, snapshot_date_key: 20250904"`

---

### B) **Silver** (dimensiones y hechos)
En *[silver](ab_nyc_dw\models\silver)* materializamos el **modelo dimensional** (dims y hechos) a partir de *staging* y/o *snapshots*.  
- `dim_borough.sql`, `dim_neighbourhood.sql`, `dim_room_type.sql`: **dimensiones conformadas** para enriquecer `ab_nyc`.  
- `dim_exchange_rate.sql`+`dim_date.sql` + `fx_rate_audit.sql`: tabla de **tipos de cambio** y trazabilidad/auditorÃ­a.  
- `dim_listing.sql`, `dim_host.sql`: entidades normalizadas desde staging usan SCD.
- `fct_listing_snapshot.sql`: **tabla de hechos** que representa el estado del *listing* en cada **snapshot_date** (grano *listing Ã— snapshot*).  

> **[dbt snapshots](ab_nyc_dw\snapshots)** (carpeta `snapshots/`):  
> - Capturan cambios **a lo largo del tiempo** en entidades como *listing* y *host* (SCD-2).  
> - Se alimentan de *staging* y escriben en tablas â€œ\_snapshotsâ€ que luego usa *silver*.  
> - Archivo en repo: [`snapshots/listing_snapshot.sql`](ab_nyc_dw\snapshots\listing_snapshot.sql), [`snapshots/host_snapshot.sql`](ab_nyc_dw\snapshots\host_snapshot.sql).

### ğŸ”¬ Tests en **Staging** y **Silver** (dbt)

AdemÃ¡s de construir los modelos, **staging** y **silver** incluyen **tests** para asegurar calidad e integridad referencial.  

#### ğŸ“ DÃ³nde viven los tests
- **Staging**: [`ab_nyc_dw/models/staging/schema.yml`](ab_nyc_dw\models\staging\schema.yml)
- **Silver**: [`ab_nyc_dw/models/silver/schema.yml`](ab_nyc_dw\models\silver\schema.yml)

> **EjecuciÃ³n recomendada (staging + snapshots + silver) con _tests_ incluidos:**
```bash
# 1) STAGING â†’ build + tests
docker compose run --rm dbt sh -lc "dbt run --select staging && dbt test --select staging"

# 2) SNAPSHOTS (SCD-2)
docker compose run --rm dbt dbt snapshot

# 3) SILVER â†’ build + tests
docker compose run --rm dbt sh -lc "dbt run --select silver && dbt test --select silver"
```
## C) ğŸ¥‡**GOLD** â€” MÃ©tricas, KPIs y agregados (dbt)

La capa **gold** contiene **vistas** orientadas al anÃ¡lisis de negocio.  
Se construyen **encima de silver** (dimensiones conformadas y la fact **`fct_listing_snapshot`**) y, cuando aplica, de **`dim_date`** y **`dim_exchange_rate`** para manejar tiempos y FX.

---

### ğŸ“‚ Estructura (rutas dentro de [`ab_nyc_dw/models/gold`](ab_nyc_dw\models\gold))

```text
ab_nyc_dw/
â””â”€ models/
   â””â”€ gold/
      â”œâ”€ gq1_price_by_area.sql
      â”œâ”€ gq2_roomtype_supply_revenue.sql
      â”œâ”€ gq3_top_hosts_pricing.sql
      â”œâ”€ gq4_availability_diffs.sql
      â”œâ”€ gq5_reviews_trend_monthly.sql
      â”œâ”€ gq6_active_listings_concentration.sql
      â”œâ”€ gq7_price_distribution_outliers.sql
      â”œâ”€ gq8_availability_vs_reviews.sql
      â”œâ”€ gq9_borough_supply_density_ranked.sql
      â””â”€ schema.yml      # tests de la capa gold (not_null, unique, etc.)
```

> **MaterializaciÃ³n**: las consultas gold se materializan como **`view`** (config en cada `.sql`).

---

### ğŸ” Modelos clave (quÃ© hace cada uno)

- **[`gq1_price_by_area.sql`](ab_nyc_dw\models\gold\gq1_price_by_area.sql)**  
  - Toma el **Ãºltimo snapshot** (`max(snapshot_date_key)`) de `fct_listing_snapshot`.  
  - Calcula precios **promedio por borough y neighbourhood** (USD).  
  - Agrega columnas revaluadas a **MXN** de dos formas:
    - **as-of**: usando el **FX vigente a la fecha del snapshot**.
    - **fx_latest**: usando el **Ãºltimo FX disponible** en `dim_exchange_rate`.  
  - Devuelve **rankings** por precio para comparar zonas.

- **[`gq2_roomtype_supply_revenue.sql`](ab_nyc_dw\models\gold\gq2_roomtype_supply_revenue.sql)**  
  - Agrega por **tipo de habitaciÃ³n**:  
    - **oferta activa** (conteo de listings activos).  
    - **proxy de revenue** en **MXN (as-of)** y **USD**, mÃ¡s **MXN revaluado** con **Ãºltimo FX**.  
  - Emplea un cÃ¡lculo simple de **noches reservadas**: `365 - availability_365` (nunca negativo).  
  - Incluye **ranks** de popularidad y revenue.

- **[`gq3_top_hosts_pricing.sql`](ab_nyc_dw\models\gold\gq3_top_hosts_pricing.sql)**  
  - Ranking de **hosts** precio / oferta activa (sobre el Ãºltimo snapshot).  
  - Ãštil para detectar proveedores con mayor impacto.

- **[`gq4_availability_diffs.sql`](ab_nyc_dw\models\gold\gq4_availability_diffs.sql)**  
  - Promedio y **mediana (p50)** de **availability_365** por **borough** y **room_type** en el **Ãºltimo snapshot**.  
  - Permite ver **brechas** de disponibilidad entre zonas y tipos de habitaciÃ³n.

- **`gq5_* â€¦ gq9_*`**  
  - **Tendencias mensuales de reseÃ±as**, **concentraciÃ³n de listings activos**, **distribuciÃ³n de precios / outliers**, **relaciÃ³n disponibilidad vs reseÃ±as**, y **ranking por densidad de oferta**.  
  - Todas se basan en **dimensiones de silver** y **`fct_listing_snapshot`**, usando `dim_date` para cortes temporales cuando aplica.

---

### âœ… Tests en GOLD

En [`ab_nyc_dw/models/gold/schema.yml`](ab_nyc_dw\models\gold\schema.yml) puedes definir tests como:

- **`not_null`** y **`unique`** en claves tÃ©cnicas.  
- **`relationships`** hacia dimensiones de silver cuando exista una FK lÃ³gica.  
- **Reglas de negocio** (tests personalizados) para asegurar rangos razonables (ej., valores positivos).

> RecomendaciÃ³n: gold suele tener tests mÃ¡s **ligeros** (sanidad y lÃ³gica), ya que la **calidad fuerte** se valida en **staging** y **silver**.


### â–¶ï¸ EjecuciÃ³n (siguiendo lo que ya tenemos)

> **Flujo sugerido completo con _tests_ por capa:**

```bash
# 1) STAGING â†’ build + tests
docker compose run --rm dbt sh -lc "dbt run --select staging && dbt test --select staging"

# 2) SNAPSHOTS (SCD-2)
docker compose run --rm dbt dbt snapshot

# 3) SILVER â†’ build + tests
docker compose run --rm dbt sh -lc "dbt run --select silver && dbt test --select silver"

# 4) GOLD â†’ build + tests
docker compose run --rm dbt sh -lc "dbt run --select gold && dbt test --select gold"

```
# â˜ï¸ OrquestaciÃ³n con **Airflow** â€” Levantar el servicio y usar el DAG `ab_nyc_elt`

Esta secciÃ³n explica, paso a paso, cÃ³mo **arrancar Airflow en contenedor**, registrar la **conexiÃ³n a Postgres**, y **ejecutar** el DAG [`ab_nyc_elt`](dags/ab_nyc_elt.py) que orquesta: **extracciÃ³n RAW â†’ actualizaciÃ³n de `latest.csv` â†’ `dbt build` â†’ chequeo en DWH**.

---

## ğŸ“ UbicaciÃ³n de archivos clave

- **DAG**: `dags/ab_nyc_elt.py`
- **Entorno**: `airflow.env` (variables de Airflow y admin)
- **Imagen**: [`airflow.Dockerfile`](airflow.Dockerfile)
- **Dependencias**: [`requirements_airflow.txt`](requirements_airflow.txt)
- **Bootstrap**: `airflow_init.sh` (migraciÃ³n DB y creaciÃ³n de usuario admin)

> Requisito de montajes (conceptual, en tu `docker-compose.yml`):  
> - `./dags` â†’ `/opt/airflow/dags`  
> - **RaÃ­z del repo** â†’ `/opt/airflow/repo` (el DAG hace `cd /opt/airflow/repo`)  
> - `./logs_airflow` â†’ `/opt/airflow/logs`  
> - `./data` â†’ `/opt/airflow/repo/data` (para que el DAG vea `data/`)

---

## â–¶ï¸ Ciclo de arranque

### 1) Construir la imagen de Airflow
```bash
docker compose build airflow
```
### 2) Inicializar metadatos + usuario admin
```bash
docker compose run --rm airflow ./airflow_init.sh
```
### 2) levantar el servicio
```bash
docker compose up -d airflow
```
Interfaz web disponible (por defecto): http://localhost:8080

### ConexiÃ³n `postgres_dw` (ejemplo genÃ©rico)

**En Airflow UI â†’ _Admin â†’ Connections_:**
- **Conn Id:** `postgres_dw`
- **Conn Type:** `Postgres`
- **Host:** `dw-host`            <!-- ej. el nombre del servicio en docker compose -->
- **Port:** `5432`
- **Schema:** `dw_database`       <!-- la base de datos donde dbt escribe/lee -->
- **Login:** `dw_user`
- **Password:** `dw_pass`

> Sugerencias:
> - Si usas Docker Compose y tu contenedor de Postgres se llama `postgres`, pon **Host = `postgres`**.
> - Presiona **Test** en la conexiÃ³n para validar que Airflow llega a la BD.

**VÃ­a CLI (URI en una lÃ­nea):**
```bash
docker compose exec airflow \
  airflow connections add postgres_dw \
  --conn-uri 'postgresql+psycopg2://dw_user:dw_pass@dw-host:5432/dw_database'
```
## ğŸ§© Â¿QuÃ© hace el DAG [`ab_nyc_elt`](dags\ab_nyc_elt.py)?

### Grafo de tareas
```text
extract_raw  â†’  update_symlinks  â†’  dbt_build  â†’  gold_has_rows
```
ğŸš€ Ejecutar el DAG
OpciÃ³n A â€” Desde la UI

1. En DAGs, busca ab_nyc_elt, ponlo en Unpause (si aplica) y pulsa Trigger DAG.

2. Abre el Graph y monitorea los logs de cada tarea.

- OpciÃ³n B â€” Desde CLI
```bash
# Disparar una corrida manual
docker compose exec airflow \
  airflow dags trigger ab_nyc_elt --run-id "manual_$(date -u +%Y%m%dT%H%M%SZ)"

# Ver el estado de las corridas
docker compose exec airflow airflow dags list-runs -d ab_nyc_elt

# Logs del servicio (scheduler/webserver) en vivo
docker compose logs -f airflow
```

# ğŸ“Š Consulta y visualizaciÃ³n manual de **GOLD** (Postgres) con **DBeaver** y **Python**

> Esta secciÃ³n va **despuÃ©s** de terminar la capa **GOLD** en dbt.  
> El DAG de Airflow aÃºn no corre de punta a punta, puedes **conectarte manualmente al DWH** (Postgres) para explorar las vistas `public_gold.*` desde **DBeaver** y tambiÃ©n con un pequeÃ±o **conector Python** para tablas y grÃ¡ficos.

---

## ğŸ§  Â¿QuÃ© hace el conector? ([`db_conector.py`](notebook\db_conector.py))

- Lee variables de conexiÃ³n desde **`.env`** (host, puerto, base, usuario, password).
- Crea un **SQLAlchemy Engine** y expone utilidades para usarlo.
- Funciones:
  - `get_db_engine()` â†’ devuelve el `Engine` 
  - `get_db_session()` â†’ devuelve una sesiÃ³n ORM 
  - `get_db_connection()` â†’ conexiÃ³n cruda 

- Archivo para conectarse para visualizar los grÃ¡ficos con anÃ¡lisis de preguntas de negocio:
- ['notebooks/db_conector.py'](notebook\db_conector.py)

- ğŸ““ Notebook donde se encuentran los graficos con anÃ¡lisis de preguntas de negocio:
- ['notebooks/02_dashboard_ELT.ipynb'](notebook\02_dashboard_ELT.ipynb)

(abre una conexiÃ³n con db_conector.py, ejecuta consultas a vistas public_gold.* y genera grÃ¡ficos con pandas/matplotlib/etc)

- AsegÃºrate de que exista tu **`.env`** con las variables esperadas por el conector.

### ğŸ“„ Variables mÃ­nimas esperadas en `.env`
```env
# Postgres (ajusta a tu entorno real)
PGHOST=localhost
PGPORT=5432
PGDATABASE=ab_nyc_dw
PGUSER=tu_usuario
PGPASSWORD=tu_password
```



